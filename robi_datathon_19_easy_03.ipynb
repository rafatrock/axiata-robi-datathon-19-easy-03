{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "robi-datathon-19-easy-03",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZYxOI1mGNV1",
        "colab_type": "text"
      },
      "source": [
        "#Step 01. Install All Dependencies\n",
        "\n",
        "This installs Apache Spark 2.3.3, Java 8, Findspark library that makes it easy for Python to work on the given Big Data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG5jn29qF91X",
        "colab_type": "code",
        "outputId": "1a3afa98-6900-448f-8961-4b17f52c4a90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "#OpenJDK Dependencies for Spark\n",
        "os.system('apt-get install openjdk-8-jdk-headless -qq > /dev/null')\n",
        "\n",
        "#Download Apache Spark\n",
        "os.system('wget -q http://apache.osuosl.org/spark/spark-2.3.3/spark-2.3.3-bin-hadoop2.7.tgz') \n",
        "\n",
        "#Apache Spark and Hadoop Unzip\n",
        "os.system('tar xf spark-2.3.3-bin-hadoop2.7.tgz')\n",
        "\n",
        "#FindSpark Install\n",
        "os.system('pip install -q findspark')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE_C9VOSHOKy",
        "colab_type": "text"
      },
      "source": [
        "# Step 02. Set Environment Variables\n",
        "Set the locations where Spark and Java are installed based on your installation configuration. Double check before you proceed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrOQoyMmHRPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.3-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY1GD4JslzUH",
        "colab_type": "text"
      },
      "source": [
        "# Step 03. ELT - Load the Data: Mega Cloud Access\n",
        "This is an alternative approach to load datasets from already stored in [**Mega Cloud**](https://mega.nz) cloud repository. You need to install the necessary packages and put the link URL of cloud to load the file from cloud directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCqsmO2fl_9Y",
        "colab_type": "code",
        "outputId": "20400bef-a75c-4a1d-eb77-bb4e30bf036c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.system('git clone https://github.com/jeroenmeulenaar/python3-mega.git')\n",
        "os.chdir('python3-mega')\n",
        "os.system('pip install -r requirements.txt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAYD5dmomHgL",
        "colab_type": "text"
      },
      "source": [
        "# Step 04. ELT - Load the Data: Read Uploaded Dataset\n",
        "In this approach you can directly load the uploaded dataset downloaded fro Mega Cloud Infrastructure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCWFVfBsmM03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mega import Mega\n",
        "os.chdir('../')\n",
        "m_prepaid = Mega.from_ephemeral()\n",
        "m_prepaid.download_from_url('https://mega.nz/#!kwRG2YyC!8TU2aoENtn98M9FHvGEjbk4iIsxBDBERw_H_wFzUchA')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqraRdPk2dPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m_postpaid = Mega.from_ephemeral()\n",
        "m_postpaid.download_from_url('https://mega.nz/#!R0RWwAKA!4HK8qDfIxCFo40mS7hnYjo5XiBYmGhUcB23rN5uSQQk')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsn-eVwy2d9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m_crm = Mega.from_ephemeral()\n",
        "m_crm.download_from_url('https://mega.nz/#!soBChSAY!gnyBRDrglAgkQffIAOqHDGGoOViYywT6eXc2DYZlz5E')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iRVypEMHiDe",
        "colab_type": "text"
      },
      "source": [
        "# Step 05. Start a Spark Session\n",
        "This basic code will prepare to start a Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrW7H-rmHmFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('ml-datathon19-easy03').master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L0hCp7CvhRv",
        "colab_type": "text"
      },
      "source": [
        "# Step 06. Data Cleaning - Phase 01 Data Schema View\n",
        "Now let's load the DataFrame and see the schema view of the Spark dataset and start cleaning our datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKExso52vzHV",
        "colab_type": "code",
        "outputId": "75ab36a4-8afd-4179-de7c-fa50c2431c58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df_crm = spark.read.csv('crm.csv', header = False, inferSchema = True)\n",
        "df_crm.dtypes"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('_c0', 'string'),\n",
              " ('_c1', 'string'),\n",
              " ('_c2', 'int'),\n",
              " ('_c3', 'string'),\n",
              " ('_c4', 'string'),\n",
              " ('_c5', 'string')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qkEyXdqz0lM",
        "colab_type": "text"
      },
      "source": [
        "# Step 07. Data Cleaning - Phase 02 Header Update\n",
        "Now let's load the DataFrame and we can see that there is no header for this dataset. So we need to update the header columns and verify "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcGevSjzxu-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_crm =df_crm.withColumnRenamed('_c0', 'msisdn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM2zs_pKzGeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_crm =df_crm.withColumnRenamed('_c1', 'gender')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz3r-95DzYu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_crm =df_crm.withColumnRenamed('_c2', 'year_of_birth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC_wfXZkzdC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_crm =df_crm.withColumnRenamed('_c3', 'system_status')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUp-IvTaziZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_crm =df_crm.withColumnRenamed('_c4', 'mobile_type')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awg-HPT-zs7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_crm =df_crm.withColumnRenamed('_c5', 'value_segment')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfye1ZI1zGUS",
        "colab_type": "code",
        "outputId": "03e1f88b-546f-4d0d-8197-7c9093b06671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "df_crm.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(msisdn='aeef4233d9ad34e41f7ecf48d64646f8', gender='MALE', year_of_birth=1985, system_status='ACTIVE', mobile_type='Prepaid', value_segment='Tier_3')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0_qrE8QwVmh",
        "colab_type": "text"
      },
      "source": [
        "# Step 08. Data Cleaning - Phase 03 Row Count and Unique Row Count\n",
        "We need to know the number of rows. We'll grab total number of entries to have an overview of all data and grab total number of unique entries or unique row count of the Spark dataset to have an overview of duplicate data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwARm9PS0V86",
        "colab_type": "code",
        "outputId": "3e0c54e1-b151-49e3-fef8-45092aeb43ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Total Rows: \")\n",
        "df_crm.count()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Rows: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97372872"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaV-8uxe2ahG",
        "colab_type": "code",
        "outputId": "0c657b76-fd0e-4cf7-9a2c-370501c1a36e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Unique Rows: \")\n",
        "df_crm.distinct().count()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique Rows: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "97355992"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBxULXKI7lLJ",
        "colab_type": "text"
      },
      "source": [
        "# Step 09. Data Cleaning - Phase 04 Removing duplicate rows\n",
        "\n",
        "Roughly `0.017%` data are `DUPLICATE` values from the table. Since this is neglitible compare to the original row count, we will now filter the dataset to remove all `DUPLICATE` values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLzWHJWE8I9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_crm = df_crm.dropDuplicates()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8FfuQpM09yn",
        "colab_type": "text"
      },
      "source": [
        "# Step 10. Data Cleaning - Phase 05 Review NULL values in each column\n",
        "Since the total row count and unique row count are now same, it means there is no duplicate rows in the table. Now we'll grab the count of NULL values per column to check whether any missing values is there or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwccPg5i05PY",
        "colab_type": "code",
        "outputId": "c774a5a7-cb07-4981-98de-4e9f4a2b84c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import pyspark.sql.functions as F\n",
        "df_agg = df_crm.agg(*[F.count(F.when(F.isnull(c), c)).alias(c) for c in df_crm.columns])\n",
        "df_agg.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------+-------------+-------------+-----------+-------------+\n",
            "|msisdn|  gender|year_of_birth|system_status|mobile_type|value_segment|\n",
            "+------+--------+-------------+-------------+-----------+-------------+\n",
            "|     0|18963284|      3206817|            1|          1|      8173367|\n",
            "+------+--------+-------------+-------------+-----------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77yuXx_l1mwQ",
        "colab_type": "text"
      },
      "source": [
        "# Step 11. Data Cleaning - Advanced Data Filling\n",
        "Roughly `19.5%` `gender` data, `3%` `year_of_birth` data and `8.5%` `value_segment` data are `NULL` values from the table. Since this is a large chunk of data, we will now do some advanced techniques to fill the required columns so that we can do our operations fruitfully"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tArst-5yEcrC",
        "colab_type": "code",
        "outputId": "a59f2b77-1a13-43ea-8fc3-9f64d7e43d08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "df_crm.groupBy(\"year_of_birth\").count().sort(\"count\", ascending=False).show() "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-------+\n",
            "|year_of_birth|  count|\n",
            "+-------------+-------+\n",
            "|         1988|4722270|\n",
            "|         1982|4382183|\n",
            "|         1987|4342609|\n",
            "|         1989|4210311|\n",
            "|         1985|4157539|\n",
            "|         1983|3476667|\n",
            "|         1980|3441825|\n",
            "|         1977|3369041|\n",
            "|         1986|3286140|\n",
            "|         null|3206817|\n",
            "|         1984|3116875|\n",
            "|         1994|3021323|\n",
            "|         1990|3003995|\n",
            "|         1972|2946266|\n",
            "|         1991|2821666|\n",
            "|         1993|2760131|\n",
            "|         1992|2744080|\n",
            "|         1981|2640796|\n",
            "|         1979|2572952|\n",
            "|         1978|2531215|\n",
            "+-------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c2a71c34-2f76-4dd5-91b0-1f50fd724aa4",
        "id": "W1ByGcp3MNqI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "df_crm_year_10 = df_crm.filter(df_crm[\"year_of_birth\"]<2001)\n",
        "df_crm_year_10_75 = df_crm_year_10.filter(df_crm_year_10[\"year_of_birth\"]>1944)\n",
        "df_crm_year_10_75.groupBy(\"year_of_birth\").count().sort(\"year_of_birth\", ascending=False).show() "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-------+\n",
            "|year_of_birth|  count|\n",
            "+-------------+-------+\n",
            "|         2000|   7333|\n",
            "|         1999| 141815|\n",
            "|         1998| 433512|\n",
            "|         1997| 897077|\n",
            "|         1996| 980577|\n",
            "|         1995|1165529|\n",
            "|         1994|3021323|\n",
            "|         1993|2760131|\n",
            "|         1992|2744080|\n",
            "|         1991|2821666|\n",
            "|         1990|3003995|\n",
            "|         1989|4210311|\n",
            "|         1988|4722270|\n",
            "|         1987|4342609|\n",
            "|         1986|3286140|\n",
            "|         1985|4157539|\n",
            "|         1984|3116875|\n",
            "|         1983|3476667|\n",
            "|         1982|4382183|\n",
            "|         1981|2640796|\n",
            "+-------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6zpKI7wvvV3",
        "colab_type": "code",
        "outputId": "53319b15-14e3-48e8-bbf6-96a39b4cf518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "df_crm_year_10_75.groupBy(\"gender\").count().sort(\"count\", ascending=False).show() "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+--------+\n",
            "|       gender|   count|\n",
            "+-------------+--------+\n",
            "|         Male|61262532|\n",
            "|         null|18725584|\n",
            "|       Female| 7472728|\n",
            "|         MALE| 3807337|\n",
            "|Not Available| 1116998|\n",
            "|       FEMALE| 1047494|\n",
            "|            M|   44518|\n",
            "|         male|   10238|\n",
            "|       female|    3029|\n",
            "|            U|    2664|\n",
            "|            F|    1508|\n",
            "|        Other|    1293|\n",
            "|            .|     642|\n",
            "|          N/A|     623|\n",
            "|            3|     244|\n",
            "|      Female.|     212|\n",
            "|            ]|     129|\n",
            "|            `|     107|\n",
            "|            +|      60|\n",
            "|            B|      59|\n",
            "+-------------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMtcHTIZIIdG",
        "colab_type": "text"
      },
      "source": [
        "#Test From Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCtA3mAo8pFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_crm_year_10_75_x = df_crm_year_10_75.replace(['M', 'm', 'MALE', 'male','Male.'], ['Male', 'Male', 'Male', 'Male', 'Male'], 'gender')\n",
        "df_crm_year_10_75_xy = df_crm_year_10_75_x.replace(['F', 'f', 'FEMALE', 'female', 'Female.', 'Female]', 'FEMELE', 'FE', 'Not Available', 'Female3'], ['Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female', 'Female'], 'gender')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOqjy09yFo1p",
        "colab_type": "code",
        "outputId": "4c4c84d7-829e-44b2-f74b-db740b06a7b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "df_crm_year_10_75_xy = df_crm_year_10_75_xy.fillna({'gender':'Female'})\n",
        "df_crm_year_10_75_xy.groupBy(\"gender\").count().sort(\"count\", ascending=False).show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+--------+\n",
            "| gender|   count|\n",
            "+-------+--------+\n",
            "|   Male|65124655|\n",
            "| Female|28367685|\n",
            "|      U|    2664|\n",
            "|  Other|    1293|\n",
            "|      .|     642|\n",
            "|    N/A|     623|\n",
            "|      3|     244|\n",
            "|      ]|     129|\n",
            "|      `|     107|\n",
            "|      +|      60|\n",
            "|      B|      59|\n",
            "|      0|      46|\n",
            "|      ~|      33|\n",
            "|     \\\\|      30|\n",
            "|      D|      28|\n",
            "|      S|      25|\n",
            "|      P|      20|\n",
            "|Female`|      17|\n",
            "|FemaleF|      13|\n",
            "|FemaleH|      12|\n",
            "+-------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL_6wTHWm5jG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b8dfe036-b20f-4763-83de-764a0f67e6c7"
      },
      "source": [
        "df_prepaid = spark.read.csv('prepaid.csv', header = True, inferSchema = True)\n",
        "df_prepaid.dtypes"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('imei_tac', 'string'),\n",
              " ('msisdn', 'string'),\n",
              " ('years', 'int'),\n",
              " ('months', 'int'),\n",
              " ('total_volume_mb', 'int'),\n",
              " ('bundle_volume_mb', 'int'),\n",
              " ('pay_per_use_volume', 'int'),\n",
              " ('free_volume_mb', 'int')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZXuhLm_nZ_s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "f40d358b-6320-48b8-bd11-a54762e709ed"
      },
      "source": [
        "df_postpaid = spark.read.csv('postpaid.csv', header = True, inferSchema = True)\n",
        "df_postpaid.dtypes"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('imei_tac', 'string'),\n",
              " ('msisdn', 'string'),\n",
              " ('years', 'int'),\n",
              " ('months', 'int'),\n",
              " ('total_volume_mb', 'int'),\n",
              " ('bundle_volume_mb', 'int'),\n",
              " ('pay_per_use_volume', 'int'),\n",
              " ('free_volume_mb', 'int')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eY5q0BoMnf_Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ta = df_crm_year_10_75_xy.alias('ta')\n",
        "tb = df_prepaid.alias('tb')\n",
        "tc = df_postpaid.alias('tc')\n",
        "inner_join_pre = ta.join(tb, ta.msisdn == tb.msisdn)\n",
        "inner_join_post = ta.join(tc, ta.msisdn == tc.msisdn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3ryOqLzoHfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "129963db-5391-414e-c72f-924ab1c1f55a"
      },
      "source": [
        "from pyspark.sql.functions import desc\n",
        "Easy02 = inner_join_pre.groupBy('gender').agg({'total_volume_mb':'sum'}).sort(desc(\"sum(total_volume_mb)\"))\n",
        "Easy02 = Easy02.withColumnRenamed(\"sum(total_volume_mb)\", \"Total Data Consumed (Prepaid)\")\n",
        "Easy02 = Easy02.withColumnRenamed(\"gender\", \"Gender\")\n",
        "Easy02.show(n=2, truncate=False)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+-----------------------------+\n",
            "|Gender|Total Data Consumed (Prepaid)|\n",
            "+------+-----------------------------+\n",
            "|Male  |107881846224                 |\n",
            "|Female|46529628579                  |\n",
            "+------+-----------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzM6I3P-ttrx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a3859777-dfc1-4748-ac89-fc56c066b49d"
      },
      "source": [
        "from pyspark.sql.functions import desc\n",
        "Easy03 = inner_join_post.groupBy('gender').agg({'total_volume_mb':'sum'}).sort(desc(\"sum(total_volume_mb)\"))\n",
        "Easy03 = Easy03.withColumnRenamed(\"sum(total_volume_mb)\", \"Total Data Consumed (Postpaid)\")\n",
        "Easy03 = Easy03.withColumnRenamed(\"gender\", \"Gender\")\n",
        "Easy03.show(n=2, truncate=False)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+------------------------------+\n",
            "|Gender|Total Data Consumed (Postpaid)|\n",
            "+------+------------------------------+\n",
            "|Male  |6804560154                    |\n",
            "|Female|2639851004                    |\n",
            "+------+------------------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCsnEZE0ED0M",
        "colab_type": "text"
      },
      "source": [
        "#Stop Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyv5qDPL2RCp",
        "colab_type": "text"
      },
      "source": [
        "# Step 06. Exploration - Data Schema View\n",
        "Now let's load the DataFrame and see the schema view of the Spark dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QIDzJSb-4nh",
        "colab_type": "text"
      },
      "source": [
        "# Step 07. Exploration - Row Count\n",
        "Now since all the rows are here string/text formatted there is no meaning of running statistical method over the values of these columns. But we need to know the number of rows. We'll grab total number of entries to have an overview of data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJWm-tno9gq0",
        "colab_type": "text"
      },
      "source": [
        "# Step 08. Exploration - Total Unique Row Count\n",
        "Now we'll grab total number of unique entries or unique row count of the Spark dataset to have an overview of duplicate data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qiqc1vi609iy",
        "colab_type": "text"
      },
      "source": [
        "# Step 09. Exploration - Reviewing the NULL values in each column\n",
        "Since the total row count and unique row count are same, it means there is no duplicate rows in the table. Now we'll grab the count of NULL values per column to check whether any missing values is there or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh7ZYyVkW5Dh",
        "colab_type": "text"
      },
      "source": [
        "# Step 10. Exploration - Filtering the NULL values rows of Model entries\n",
        "Roughly `1.25%` data are `NULL` values from the table. Since this is neglitible compare to the original row count, we will now filter the dataset to remove all `NULL` values of `model_name` column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZfgybnr95iA",
        "colab_type": "text"
      },
      "source": [
        "# Step 11. Implementation - Run the SQL Command\n",
        "Now since we got the idea that there is no NULL values and we optimises the dual SIM enabled mobile set rows, we can straightly go for executing SQL command to get the desired outcome. As a part of optimisation, we can drop of the column week_number as this is not relevant to this problem."
      ]
    }
  ]
}